import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Thiáº¿t láº­p font vÃ  style
plt.rcParams['font.family'] = ['DejaVu Sans']
plt.style.use('default')

print('ğŸ“Š Táº O BIá»‚U Äá»’ FEATURE IMPORTANCE')
print('=' * 50)

# Dá»¯ liá»‡u tá»« káº¿t quáº£ training (láº¥y tá»« káº¿t quáº£ Ä‘Ã£ cháº¡y)
features = ['total_actions', 'unique_products', 'total_spending', 'avg_spending', 'age', 'income_encoded', 'education_encoded']

# Feature importance tá»« Random Forest
rf_importance = [0.067877, 0.069348, 0.324712, 0.293655, 0.159855, 0.034851, 0.049702]

# Feature importance tá»« Gradient Boosting
gb_importance = [0.000516, 0.020839, 0.502674, 0.309984, 0.126699, 0.012385, 0.026904]

# Feature coefficients tá»« Logistic Regression (absolute values)
lr_coefficients = [0.238890, 0.275727, 0.000006, 0.000014, 0.067497, 0.080019, 0.215671]

# Táº¡o figure vá»›i 2 subplot
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))

# 1. Random Forest Feature Importance
colors_rf = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD', '#98D8C8']
bars1 = ax1.barh(features, rf_importance, color=colors_rf)
ax1.set_title('Random Forest - Feature Importance', fontsize=16, fontweight='bold', pad=20)
ax1.set_xlabel('Importance Score', fontsize=12)
ax1.set_ylabel('Features', fontsize=12)

# ThÃªm giÃ¡ trá»‹ lÃªn cá»™t
for i, (bar, value) in enumerate(zip(bars1, rf_importance)):
    ax1.text(value + 0.005, bar.get_y() + bar.get_height()/2, 
             f'{value:.3f}', va='center', ha='left', fontweight='bold')

# 2. Gradient Boosting Feature Importance
colors_gb = ['#FF9FF3', '#54A0FF', '#5F27CD', '#00D2D3', '#FF9F43', '#EE5A24', '#C44569']
bars2 = ax2.barh(features, gb_importance, color=colors_gb)
ax2.set_title('Gradient Boosting - Feature Importance', fontsize=16, fontweight='bold', pad=20)
ax2.set_xlabel('Importance Score', fontsize=12)
ax2.set_ylabel('Features', fontsize=12)

# ThÃªm giÃ¡ trá»‹ lÃªn cá»™t
for i, (bar, value) in enumerate(zip(bars2, gb_importance)):
    ax2.text(value + 0.005, bar.get_y() + bar.get_height()/2, 
             f'{value:.3f}', va='center', ha='left', fontweight='bold')

plt.tight_layout()
plt.savefig('feature_importance_comparison.png', dpi=300, bbox_inches='tight')
print('âœ… ÄÃ£ lÆ°u biá»ƒu Ä‘á»“ so sÃ¡nh: feature_importance_comparison.png')

# Táº¡o biá»ƒu Ä‘á»“ tá»•ng há»£p
fig2, ax = plt.subplots(1, 1, figsize=(15, 10))

# Táº¡o DataFrame Ä‘á»ƒ dá»… váº½
df_importance = pd.DataFrame({
    'Random Forest': rf_importance,
    'Gradient Boosting': gb_importance,
    'Logistic Regression': lr_coefficients
}, index=features)

# Váº½ biá»ƒu Ä‘á»“ grouped bar
x = np.arange(len(features))
width = 0.25

bars1 = ax.bar(x - width, df_importance['Random Forest'], width, label='Random Forest', color='#FF6B6B', alpha=0.8)
bars2 = ax.bar(x, df_importance['Gradient Boosting'], width, label='Gradient Boosting', color='#4ECDC4', alpha=0.8)
bars3 = ax.bar(x + width, df_importance['Logistic Regression'], width, label='Logistic Regression', color='#45B7D1', alpha=0.8)

ax.set_xlabel('Features', fontsize=12, fontweight='bold')
ax.set_ylabel('Importance Score', fontsize=12, fontweight='bold')
ax.set_title('So sÃ¡nh Feature Importance giá»¯a cÃ¡c mÃ´ hÃ¬nh', fontsize=16, fontweight='bold', pad=20)
ax.set_xticks(x)
ax.set_xticklabels(features, rotation=45, ha='right')
ax.legend(fontsize=12)
ax.grid(True, alpha=0.3)

# ThÃªm giÃ¡ trá»‹ lÃªn cá»™t
def add_value_labels(bars):
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.001,
                f'{height:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')

add_value_labels(bars1)
add_value_labels(bars2)
add_value_labels(bars3)

plt.tight_layout()
plt.savefig('feature_importance_all_models.png', dpi=300, bbox_inches='tight')
print('âœ… ÄÃ£ lÆ°u biá»ƒu Ä‘á»“ tá»•ng há»£p: feature_importance_all_models.png')

# Táº¡o biá»ƒu Ä‘á»“ top features
fig3, ax = plt.subplots(1, 1, figsize=(12, 8))

# TÃ­nh Ä‘iá»ƒm trung bÃ¬nh cá»§a táº¥t cáº£ mÃ´ hÃ¬nh
avg_importance = (np.array(rf_importance) + np.array(gb_importance) + np.array(lr_coefficients)) / 3

# Sáº¯p xáº¿p theo thá»© tá»± giáº£m dáº§n
sorted_indices = np.argsort(avg_importance)[::-1]
sorted_features = [features[i] for i in sorted_indices]
sorted_importance = [avg_importance[i] for i in sorted_indices]

# Váº½ biá»ƒu Ä‘á»“
colors = plt.cm.viridis(np.linspace(0, 1, len(features)))
bars = ax.barh(sorted_features, sorted_importance, color=colors)

ax.set_title('Top Features - Trung bÃ¬nh tá»« táº¥t cáº£ mÃ´ hÃ¬nh', fontsize=16, fontweight='bold', pad=20)
ax.set_xlabel('Average Importance Score', fontsize=12, fontweight='bold')
ax.set_ylabel('Features', fontsize=12, fontweight='bold')

# ThÃªm giÃ¡ trá»‹ lÃªn cá»™t
for i, (bar, value) in enumerate(zip(bars, sorted_importance)):
    ax.text(value + 0.01, bar.get_y() + bar.get_height()/2, 
            f'{value:.3f}', va='center', ha='left', fontweight='bold')

# ThÃªm grid
ax.grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.savefig('top_features_average.png', dpi=300, bbox_inches='tight')
print('âœ… ÄÃ£ lÆ°u biá»ƒu Ä‘á»“ top features: top_features_average.png')

# In káº¿t quáº£ phÃ¢n tÃ­ch
print('\n=== PHÃ‚N TÃCH FEATURE IMPORTANCE ===')
print('\nğŸ† TOP 3 FEATURES QUAN TRá»ŒNG NHáº¤T:')
for i, (feature, importance) in enumerate(zip(sorted_features[:3], sorted_importance[:3]), 1):
    print(f'{i}. {feature}: {importance:.3f}')

print('\nğŸ“Š PHÃ‚N TÃCH CHI TIáº¾T:')
for i, feature in enumerate(features):
    rf_val = rf_importance[i]
    gb_val = gb_importance[i]
    lr_val = lr_coefficients[i]
    avg_val = avg_importance[i]
    print(f'{feature}:')
    print(f'  - Random Forest: {rf_val:.3f}')
    print(f'  - Gradient Boosting: {gb_val:.3f}')
    print(f'  - Logistic Regression: {lr_val:.3f}')
    print(f'  - Trung bÃ¬nh: {avg_val:.3f}')
    print()

plt.show()
print('\nğŸ‰ HoÃ n thÃ nh táº¡o biá»ƒu Ä‘á»“ Feature Importance!')
