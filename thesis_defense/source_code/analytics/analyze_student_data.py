#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ph√¢n t√≠ch d·ªØ li·ªáu sinh vi√™n t·ª´ user_actions_students_576.csv
v√† t·∫°o c√°c bi·ªÉu ƒë·ªì tr·ª±c quan
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report
import joblib
import warnings
warnings.filterwarnings('ignore')

# Thi·∫øt l·∫≠p style
plt.style.use('default')
sns.set_palette("husl")

print('=' * 70)
print('PH√ÇN T√çCH D·ªÆ LI·ªÜU SINH VI√äN')
print('=' * 70)

# ƒê·ªçc d·ªØ li·ªáu
df = pd.read_csv('../user_actions_students_576.csv')
print(f'\nƒê·ªçc d·ªØ li·ªáu: {len(df)} records, {df["user_id"].nunique()} users')

# T·∫°o features t·ª´ d·ªØ li·ªáu
user_behavior = df.groupby('user_id').agg({
    'event_type': lambda x: 'purchase' in x.values,  # Target: c√≥ mua h√†ng kh√¥ng
    'product_id': 'nunique',  # S·ªë s·∫£n ph·∫©m kh√°c nhau
    'price': ['sum', 'mean'],  # T·ªïng v√† trung b√¨nh chi ti√™u
    'age': 'first',
    'income_level': 'first',
    'education': 'first'
})

user_behavior['total_actions'] = df.groupby('user_id')['event_type'].count()
user_behavior.columns = ['is_potential', 'unique_products', 'total_spending', 'avg_spending', 'age', 'income_level', 'education', 'total_actions']

print(f'Kh√°ch h√†ng ti·ªÅm nƒÉng: {user_behavior["is_potential"].sum()}/{len(user_behavior)} ({user_behavior["is_potential"].mean()*100:.1f}%)')

# 1. PH√ÇN T√çCH TH·ªêNG K√ä C∆† B·∫¢N
print('\n' + '=' * 70)
print('1. TH·ªêNG K√ä C∆† B·∫¢N')
print('=' * 70)
print(user_behavior[['age', 'total_spending', 'avg_spending', 'total_actions', 'unique_products']].describe().round(2))

# 2. PH√ÇN T√çCH THEO ƒê·ªò TU·ªîI
print('\n' + '=' * 70)
print('2. PH√ÇN T√çCH THEO ƒê·ªò TU·ªîI')
print('=' * 70)
age_stats = user_behavior.groupby('age').agg({
    'is_potential': ['count', 'sum', 'mean']
}).round(3)
print(age_stats)

# 3. PH√ÇN T√çCH THEO TR√åNH ƒê·ªò H·ªåC V·∫§N
print('\n' + '=' * 70)
print('3. PH√ÇN T√çCH THEO TR√åNH ƒê·ªò H·ªåC V·∫§N')
print('=' * 70)
education_stats = user_behavior.groupby('education').agg({
    'is_potential': ['count', 'sum', 'mean'],
    'total_spending': 'mean',
    'avg_spending': 'mean'
}).round(2)
print(education_stats)

# 4. PH√ÇN T√çCH THEO M·ª®C THU NH·∫¨P
print('\n' + '=' * 70)
print('4. PH√ÇN T√çCH THEO M·ª®C THU NH·∫¨P')
print('=' * 70)
income_stats = user_behavior.groupby('income_level').agg({
    'is_potential': ['count', 'sum', 'mean'],
    'total_spending': 'mean',
    'avg_spending': 'mean'
}).round(2)
print(income_stats)

# Encode categorical variables
le_income = LabelEncoder()
le_education = LabelEncoder()
user_behavior['income_encoded'] = le_income.fit_transform(user_behavior['income_level'])
user_behavior['education_encoded'] = le_education.fit_transform(user_behavior['education'])

# Features
feature_columns = ['total_actions', 'unique_products', 'total_spending', 'avg_spending', 'age', 'income_encoded', 'education_encoded']
X = user_behavior[feature_columns]
y = user_behavior['is_potential']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

print(f'\nTraining set: {len(X_train)} users')
print(f'Test set: {len(X_test)} users')

# 5. TRAINING V√Ä ƒê√ÅNH GI√Å M√î H√åNH
print('\n' + '=' * 70)
print('5. TRAINING V√Ä ƒê√ÅNH GI√Å C√ÅC M√î H√åNH')
print('=' * 70)

models = {
    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100),
    'SVM': SVC(random_state=42, probability=True)
}

results = {}
y_preds = {}
y_probas = {}

for name, model in models.items():
    print(f'\nTraining {name}...')
    
    # Training
    if name == 'SVM':
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
        y_proba = model.predict_proba(X_test_scaled)[:, 1]
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        y_proba = model.predict_proba(X_test)[:, 1]
    
    # Metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, zero_division=0)
    recall = recall_score(y_test, y_pred, zero_division=0)
    f1 = f1_score(y_test, y_pred, zero_division=0)
    
    results[name] = {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'model': model
    }
    
    y_preds[name] = y_pred
    y_probas[name] = y_proba
    
    print(f'Accuracy: {accuracy:.4f}')
    print(f'Precision: {precision:.4f}')
    print(f'Recall: {recall:.4f}')
    print(f'F1-score: {f1:.4f}')

# T√¨m m√¥ h√¨nh t·ªët nh·∫•t
best_model_name = max(results.keys(), key=lambda x: results[x]['f1_score'])
print(f'\nüèÜ M√î H√åNH T·ªêT NH·∫§T: {best_model_name}')
print(f'   F1-score: {results[best_model_name]["f1_score"]:.4f}')
print(f'   Accuracy: {results[best_model_name]["accuracy"]:.4f}')

# L∆∞u m√¥ h√¨nh t·ªët nh·∫•t
best_model = results[best_model_name]['model']
joblib.dump(best_model, 'best_student_model.pkl')
print(f'\n‚úÖ ƒê√£ l∆∞u m√¥ h√¨nh t·ªët nh·∫•t: best_student_model.pkl')

# 6. T·∫†O C√ÅC BI·ªÇU ƒê·ªí
print('\n' + '=' * 70)
print('6. T·∫†O C√ÅC BI·ªÇU ƒê·ªí TR·ª∞C QUAN')
print('=' * 70)

# 6.1. Bi·ªÉu ƒë·ªì ph√¢n ph·ªëi ƒë·ªô tu·ªïi
fig, axes = plt.subplots(2, 2, figsize=(15, 12))
fig.suptitle('Ph√¢n T√≠ch D·ªØ Li·ªáu Sinh Vi√™n - Kh√°ch H√†ng Ti·ªÅm NƒÉng', fontsize=16, fontweight='bold')

# Age distribution
axes[0, 0].hist([user_behavior[user_behavior['is_potential'] == True]['age'],
                 user_behavior[user_behavior['is_potential'] == False]['age']],
                bins=20, label=['Ti·ªÅm nƒÉng', 'Kh√¥ng ti·ªÅm nƒÉng'], alpha=0.7)
axes[0, 0].set_xlabel('Tu·ªïi')
axes[0, 0].set_ylabel('S·ªë l∆∞·ª£ng')
axes[0, 0].set_title('Ph√¢n ph·ªëi theo ƒë·ªô tu·ªïi')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# Spending distribution
axes[0, 1].hist([user_behavior[user_behavior['is_potential'] == True]['total_spending'],
                 user_behavior[user_behavior['is_potential'] == False]['total_spending']],
                bins=20, label=['Ti·ªÅm nƒÉng', 'Kh√¥ng ti·ªÅm nƒÉng'], alpha=0.7)
axes[0, 1].set_xlabel('T·ªïng chi ti√™u')
axes[0, 1].set_ylabel('S·ªë l∆∞·ª£ng')
axes[0, 1].set_title('Ph√¢n ph·ªëi theo chi ti√™u')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

# Education vs Potential
education_pivot = user_behavior.groupby(['education', 'is_potential']).size().unstack(fill_value=0)
education_pivot.plot(kind='bar', ax=axes[1, 0], alpha=0.7)
axes[1, 0].set_xlabel('Tr√¨nh ƒë·ªô h·ªçc v·∫•n')
axes[1, 0].set_ylabel('S·ªë l∆∞·ª£ng')
axes[1, 0].set_title('Ph√¢n ph·ªëi theo tr√¨nh ƒë·ªô h·ªçc v·∫•n')
axes[1, 0].legend(['Kh√¥ng ti·ªÅm nƒÉng', 'Ti·ªÅm nƒÉng'])
axes[1, 0].tick_params(axis='x', rotation=45)
axes[1, 0].grid(True, alpha=0.3)

# Income level vs Potential
income_pivot = user_behavior.groupby(['income_level', 'is_potential']).size().unstack(fill_value=0)
income_pivot.plot(kind='bar', ax=axes[1, 1], alpha=0.7)
axes[1, 1].set_xlabel('M·ª©c thu nh·∫≠p')
axes[1, 1].set_ylabel('S·ªë l∆∞·ª£ng')
axes[1, 1].set_title('Ph√¢n ph·ªëi theo thu nh·∫≠p')
axes[1, 1].legend(['Kh√¥ng ti·ªÅm nƒÉng', 'Ti·ªÅm nƒÉng'])
axes[1, 1].tick_params(axis='x', rotation=45)
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('eda_plots.png', dpi=300, bbox_inches='tight')
print('‚úÖ ƒê√£ l∆∞u: eda_plots.png')

# 6.2. Correlation matrix
plt.figure(figsize=(10, 8))
corr_data = user_behavior[['is_potential', 'age', 'total_spending', 'avg_spending', 'total_actions', 'unique_products']].astype(float)
correlation = corr_data.corr()
sns.heatmap(correlation, annot=True, fmt='.2f', cmap='coolwarm', center=0,
            square=True, linewidths=1, cbar_kws={"shrink": 0.8})
plt.title('Ma Tr·∫≠n T∆∞∆°ng Quan Gi·ªØa C√°c Bi·∫øn', fontsize=14, fontweight='bold', pad=20)
plt.tight_layout()
plt.savefig('correlation_matrix.png', dpi=300, bbox_inches='tight')
print('‚úÖ ƒê√£ l∆∞u: correlation_matrix.png')

# 6.3. Model comparison
plt.figure(figsize=(12, 6))

# Plot model performance
metrics_df = pd.DataFrame(results).T
metrics_df = metrics_df[['accuracy', 'precision', 'recall', 'f1_score']]

x = np.arange(len(metrics_df.index))
width = 0.2

plt.bar(x - 1.5*width, metrics_df['accuracy'], width, label='Accuracy', alpha=0.8)
plt.bar(x - 0.5*width, metrics_df['precision'], width, label='Precision', alpha=0.8)
plt.bar(x + 0.5*width, metrics_df['recall'], width, label='Recall', alpha=0.8)
plt.bar(x + 1.5*width, metrics_df['f1_score'], width, label='F1-score', alpha=0.8)

plt.xlabel('M√¥ h√¨nh', fontsize=12, fontweight='bold')
plt.ylabel('Score', fontsize=12, fontweight='bold')
plt.title('So S√°nh Hi·ªáu Su·∫•t C√°c M√¥ H√¨nh ML', fontsize=14, fontweight='bold')
plt.xticks(x, metrics_df.index, rotation=45, ha='right')
plt.legend()
plt.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')
print('‚úÖ ƒê√£ l∆∞u: model_comparison.png')

# 6.4. Feature importance (cho Random Forest)
if 'Random Forest' in results:
    plt.figure(figsize=(10, 6))
    rf_model = results['Random Forest']['model']
    feature_importance = pd.DataFrame({
        'feature': feature_columns,
        'importance': rf_model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    plt.barh(range(len(feature_importance)), feature_importance['importance'], alpha=0.7)
    plt.yticks(range(len(feature_importance)), feature_importance['feature'])
    plt.xlabel('Importance', fontsize=12, fontweight='bold')
    plt.ylabel('Feature', fontsize=12, fontweight='bold')
    plt.title('Feature Importance - Random Forest', fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3, axis='x')
    plt.tight_layout()
    plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')
    print('‚úÖ ƒê√£ l∆∞u: feature_importance.png')
    
    print('\nüìä Feature Importance:')
    print(feature_importance.to_string(index=False))

# 6.5. Confusion Matrix cho m√¥ h√¨nh t·ªët nh·∫•t
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_preds[best_model_name])
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Kh√¥ng ti·ªÅm nƒÉng', 'Ti·ªÅm nƒÉng'],
            yticklabels=['Kh√¥ng ti·ªÅm nƒÉng', 'Ti·ªÅm nƒÉng'])
plt.title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold', pad=20)
plt.xlabel('D·ª± ƒëo√°n', fontsize=12, fontweight='bold')
plt.ylabel('Th·ª±c t·∫ø', fontsize=12, fontweight='bold')
plt.tight_layout()
plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')
print('‚úÖ ƒê√£ l∆∞u: confusion_matrix.png')

# 7. L∆ØU K·∫æT QU·∫¢
print('\n' + '=' * 70)
print('7. L∆ØU K·∫æT QU·∫¢')
print('=' * 70)

# L∆∞u k·∫øt qu·∫£ m√¥ h√¨nh
results_df = pd.DataFrame(results).T
results_df.to_csv('model_evaluation_results.csv')
print('‚úÖ ƒê√£ l∆∞u: model_evaluation_results.csv')

# L∆∞u b√°o c√°o chi ti·∫øt
with open('model_evaluation_report.txt', 'w', encoding='utf-8') as f:
    f.write('=' * 70 + '\n')
    f.write('B√ÅO C√ÅO PH√ÇN T√çCH D·ªÆ LI·ªÜU SINH VI√äN\n')
    f.write('=' * 70 + '\n\n')
    
    f.write(f'Dataset: {len(df)} records, {df["user_id"].nunique()} users\n')
    f.write(f'Kh√°ch h√†ng ti·ªÅm nƒÉng: {user_behavior["is_potential"].sum()}/{len(user_behavior)} ({user_behavior["is_potential"].mean()*100:.1f}%)\n\n')
    
    f.write('TH·ªêNG K√ä C∆† B·∫¢N:\n')
    f.write(str(user_behavior[['age', 'total_spending', 'avg_spending', 'total_actions', 'unique_products']].describe().round(2)) + '\n\n')
    
    f.write('K·∫æT QU·∫¢ M√î H√åNH:\n')
    f.write(results_df.to_string() + '\n\n')
    
    f.write(f'M√î H√åNH T·ªêT NH·∫§T: {best_model_name}\n')
    f.write(f'F1-score: {results[best_model_name]["f1_score"]:.4f}\n')
    f.write(f'Accuracy: {results[best_model_name]["accuracy"]:.4f}\n')
    
print('‚úÖ ƒê√£ l∆∞u: model_evaluation_report.txt')

print('\n' + '=' * 70)
print('HO√ÄN TH√ÄNH PH√ÇN T√çCH!')
print('=' * 70)
print('\nüìÅ C√°c file ƒë√£ ƒë∆∞·ª£c t·∫°o:')
print('   ‚Ä¢ eda_plots.png - Ph√¢n t√≠ch d·ªØ li·ªáu kh√°m ph√°')
print('   ‚Ä¢ correlation_matrix.png - Ma tr·∫≠n t∆∞∆°ng quan')
print('   ‚Ä¢ model_comparison.png - So s√°nh c√°c m√¥ h√¨nh')
print('   ‚Ä¢ feature_importance.png - T·∫ßm quan tr·ªçng c·ªßa features')
print('   ‚Ä¢ confusion_matrix.png - Ma tr·∫≠n nh·∫ßm l·∫´n')
print('   ‚Ä¢ model_evaluation_results.csv - K·∫øt qu·∫£ ƒë√°nh gi√°')
print('   ‚Ä¢ model_evaluation_report.txt - B√°o c√°o chi ti·∫øt')
print('   ‚Ä¢ best_student_model.pkl - M√¥ h√¨nh t·ªët nh·∫•t')
print(f'\nüèÜ M√¥ h√¨nh t·ªët nh·∫•t: {best_model_name} (F1-score: {results[best_model_name]["f1_score"]:.4f})')

