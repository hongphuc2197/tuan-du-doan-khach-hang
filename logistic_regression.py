import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

print('üöÄ TRAINING LOGISTIC REGRESSION TR√äN DATASET C·ª¶A B·∫†N')
print('=' * 60)

# B∆∞·ªõc 1: ƒê·ªçc dataset
print('B∆∞·ªõc 1: ƒê·ªçc dataset...')
df = pd.read_csv('user_actions_students_576.csv')
print(f'‚úÖ Dataset: {len(df)} records, {df["user_id"].nunique()} users')

# B∆∞·ªõc 2: T·∫°o features
print('\nB∆∞·ªõc 2: T·∫°o features...')
user_behavior = df.groupby('user_id').agg({
    'event_type': lambda x: 'purchase' in x.values,  # Target: c√≥ mua h√†ng kh√¥ng
    'product_id': 'nunique',  # S·ªë s·∫£n ph·∫©m kh√°c nhau
    'price': ['sum', 'mean'],  # T·ªïng v√† trung b√¨nh chi ti√™u
    'age': 'first',
    'income_level': 'first',
    'education': 'first'
})

user_behavior['total_actions'] = df.groupby('user_id')['event_type'].count()
user_behavior.columns = ['is_potential', 'unique_products', 'total_spending', 'avg_spending', 'age', 'income_level', 'education', 'total_actions']

print(f'‚úÖ Features t·∫°o xong: {len(user_behavior)} users')
print(f'‚úÖ Kh√°ch h√†ng ti·ªÅm nƒÉng: {user_behavior["is_potential"].sum()}/{len(user_behavior)} ({user_behavior["is_potential"].mean()*100:.1f}%)')

# B∆∞·ªõc 3: Encode categorical variables
print('\nB∆∞·ªõc 3: Encode categorical variables...')
le_income = LabelEncoder()
le_education = LabelEncoder()
user_behavior['income_encoded'] = le_income.fit_transform(user_behavior['income_level'])
user_behavior['education_encoded'] = le_education.fit_transform(user_behavior['education'])

print(f'‚úÖ Income levels: {list(le_income.classes_)}')
print(f'‚úÖ Education levels: {list(le_education.classes_)}')

# B∆∞·ªõc 4: Chu·∫©n b·ªã X v√† y
print('\nB∆∞·ªõc 4: Chu·∫©n b·ªã d·ªØ li·ªáu...')
feature_columns = ['total_actions', 'unique_products', 'total_spending', 'avg_spending', 'age', 'income_encoded', 'education_encoded']
X = user_behavior[feature_columns]
y = user_behavior['is_potential']

print(f'‚úÖ Features: {feature_columns}')
print(f'‚úÖ Target distribution: {y.value_counts().to_dict()}')

# B∆∞·ªõc 5: Train-test split
print('\nB∆∞·ªõc 5: Chia d·ªØ li·ªáu train/test...')
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
print(f'‚úÖ Training set: {len(X_train)} users')
print(f'‚úÖ Test set: {len(X_test)} users')

# B∆∞·ªõc 6: Training Logistic Regression
print('\nB∆∞·ªõc 6: Training Logistic Regression...')
model = LogisticRegression(random_state=42, max_iter=1000)
model.fit(X_train, y_train)
print('‚úÖ Training ho√†n th√†nh!')

# B∆∞·ªõc 7: D·ª± ƒëo√°n v√† ƒë√°nh gi√°
print('\nB∆∞·ªõc 7: D·ª± ƒëo√°n v√† ƒë√°nh gi√°...')
y_pred = model.predict(X_test)

# Metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f'\n=== K·∫æT QU·∫¢ LOGISTIC REGRESSION ===')
print(f'Accuracy: {accuracy:.4f} ({accuracy*100:.1f}%)')
print(f'Precision: {precision:.4f} ({precision*100:.1f}%)')
print(f'Recall: {recall:.4f} ({recall*100:.1f}%)')
print(f'F1-score: {f1:.4f} ({f1*100:.1f}%)')

print(f'\n=== CLASSIFICATION REPORT ===')
print(classification_report(y_test, y_pred))

# Feature importance
print(f'\n=== FEATURE IMPORTANCE ===')
feature_importance = pd.DataFrame({
    'feature': feature_columns,
    'coefficient': model.coef_[0]
}).sort_values('coefficient', key=abs, ascending=False)

print(feature_importance)

print(f'\n‚úÖ Ho√†n th√†nh training Logistic Regression!')
