#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
T·∫°o c√°c bi·ªÉu ƒë·ªì cho Ch∆∞∆°ng 5 - ƒê√°nh gi√° m√¥ h√¨nh
- H√¨nh 5.2: ROC Curve so s√°nh 4 m√¥ h√¨nh
- H√¨nh 5.3: Confusion Matrix c·ªßa m√¥ h√¨nh t·ªët nh·∫•t
- H√¨nh 5.4: Feature Importance analysis
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, 
    confusion_matrix, classification_report, roc_auc_score, roc_curve
)
import warnings
warnings.filterwarnings('ignore')

# Thi·∫øt l·∫≠p style cho matplotlib
plt.style.use('default')
sns.set_palette("husl")

def load_and_prepare_data():
    """T·∫£i v√† chu·∫©n b·ªã d·ªØ li·ªáu"""
    print("=== T·∫¢I V√Ä CHU·∫®N B·ªä D·ªÆ LI·ªÜU ===")
    
    # T·∫°o d·ªØ li·ªáu m·∫´u ƒë·ªÉ demo
    print("T·∫°o d·ªØ li·ªáu m·∫´u ƒë·ªÉ demo...")
    np.random.seed(42)
    n_samples = 1000
    
    # T·∫°o d·ªØ li·ªáu m·∫´u
    data = {
        'Year_Birth': np.random.randint(1970, 2000, n_samples),
        'Income': np.random.normal(50000, 20000, n_samples).astype(int),
        'Kidhome': np.random.randint(0, 3, n_samples),
        'Teenhome': np.random.randint(0, 3, n_samples),
        'Recency': np.random.randint(0, 100, n_samples),
        'MntWines': np.random.exponential(200, n_samples).astype(int),
        'MntFruits': np.random.exponential(50, n_samples).astype(int),
        'MntMeatProducts': np.random.exponential(150, n_samples).astype(int),
        'MntFishProducts': np.random.exponential(30, n_samples).astype(int),
        'MntSweetProducts': np.random.exponential(40, n_samples).astype(int),
        'MntGoldProds': np.random.exponential(100, n_samples).astype(int),
        'NumDealsPurchases': np.random.poisson(2, n_samples),
        'NumWebPurchases': np.random.poisson(4, n_samples),
        'NumCatalogPurchases': np.random.poisson(2, n_samples),
        'NumStorePurchases': np.random.poisson(5, n_samples),
        'NumWebVisitsMonth': np.random.poisson(6, n_samples),
        'Education': np.random.choice(['Basic', 'Graduation', 'Master', 'PhD'], n_samples),
        'Marital_Status': np.random.choice(['Single', 'Married', 'Divorced', 'Together'], n_samples)
    }
    
    df = pd.DataFrame(data)
    
    # T·∫°o target variable d·ª±a tr√™n c√°c features
    # Kh√°ch h√†ng ti·ªÅm nƒÉng c√≥ xu h∆∞·ªõng c√≥ thu nh·∫≠p cao, chi ti√™u nhi·ªÅu, v√† mua h√†ng th∆∞·ªùng xuy√™n
    potential_score = (
        df['Income'] / 1000 +  # Thu nh·∫≠p cao
        df['MntWines'] / 100 +  # Chi ti√™u r∆∞·ª£u vang
        df['MntMeatProducts'] / 100 +  # Chi ti√™u th·ªãt
        df['NumStorePurchases'] +  # Mua h√†ng t·∫°i c·ª≠a h√†ng
        df['NumWebPurchases']  # Mua h√†ng online
    )
    
    # T·∫°o Response d·ª±a tr√™n potential_score
    threshold = potential_score.quantile(0.7)  # Top 30% l√† kh√°ch h√†ng ti·ªÅm nƒÉng
    df['Response'] = (potential_score > threshold).astype(int)
    
    print(f"Dataset c√≥ {len(df)} d√≤ng v√† {len(df.columns)} c·ªôt")
    print(f"S·ªë kh√°ch h√†ng ti·ªÅm nƒÉng: {(df['Response'] == 1).sum()}")
    print(f"T·ª∑ l·ªá kh√°ch h√†ng ti·ªÅm nƒÉng: {(df['Response'] == 1).mean()*100:.2f}%")
    
    return df

def prepare_features(df):
    """Chu·∫©n b·ªã features cho training"""
    print("\n=== CHU·∫®N B·ªä FEATURES ===")
    
    # Ch·ªçn features quan tr·ªçng
    feature_columns = [
        'Year_Birth', 'Income', 'Kidhome', 'Teenhome', 'Recency',
        'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 
        'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases',
        'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 
        'NumWebVisitsMonth'
    ]
    
    # X·ª≠ l√Ω categorical variables
    le_education = LabelEncoder()
    le_marital = LabelEncoder()
    
    df['Education_encoded'] = le_education.fit_transform(df['Education'])
    df['Marital_Status_encoded'] = le_marital.fit_transform(df['Marital_Status'])
    
    feature_columns.extend(['Education_encoded', 'Marital_Status_encoded'])
    
    X = df[feature_columns]
    y = df['Response']
    
    print(f"S·ªë features: {len(feature_columns)}")
    
    return X, y, feature_columns

def train_models(X, y):
    """Training c√°c m√¥ h√¨nh"""
    print("\n=== TRAINING C√ÅC M√î H√åNH ===")
    
    # Chia d·ªØ li·ªáu
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # Chu·∫©n h√≥a d·ªØ li·ªáu
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # ƒê·ªãnh nghƒ©a c√°c m√¥ h√¨nh
    models = {
        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
        'SVM': SVC(random_state=42, probability=True)
    }
    
    results = {}
    
    for name, model in models.items():
        print(f"Training {name}...")
        
        # Training
        if name == 'SVM':
            model.fit(X_train_scaled, y_train)
            y_pred = model.predict(X_test_scaled)
            y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
        else:
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            y_pred_proba = model.predict_proba(X_test)[:, 1]
        
        # T√≠nh to√°n metrics
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, zero_division=0)
        recall = recall_score(y_test, y_pred, zero_division=0)
        f1 = f1_score(y_test, y_pred, zero_division=0)
        roc_auc = roc_auc_score(y_test, y_pred_proba)
        
        # L∆∞u k·∫øt qu·∫£
        results[name] = {
            'model': model,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'roc_auc': roc_auc,
            'y_test': y_test,
            'y_pred': y_pred,
            'y_pred_proba': y_pred_proba
        }
        
        print(f"  Accuracy: {accuracy:.4f}")
        print(f"  F1-score: {f1:.4f}")
        print(f"  ROC AUC: {roc_auc:.4f}")
    
    return results

def create_figure_5_2_roc_curves(results):
    """T·∫°o H√¨nh 5.2 - ROC Curve so s√°nh 4 m√¥ h√¨nh"""
    print("\n=== T·∫†O H√åNH 5.2 - ROC CURVES ===")
    
    plt.figure(figsize=(12, 8))
    
    # M√†u s·∫Øc cho t·ª´ng m√¥ h√¨nh
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']
    
    for i, (name, metrics) in enumerate(results.items()):
        fpr, tpr, _ = roc_curve(metrics['y_test'], metrics['y_pred_proba'])
        plt.plot(fpr, tpr, 
                color=colors[i], 
                linewidth=2.5,
                label=f'{name} (AUC = {metrics["roc_auc"]:.3f})')
    
    # ƒê∆∞·ªùng ch√©o (random classifier)
    plt.plot([0, 1], [0, 1], 'k--', linewidth=1.5, alpha=0.7, label='Random Classifier')
    
    # Thi·∫øt l·∫≠p bi·ªÉu ƒë·ªì
    plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=12, fontweight='bold')
    plt.ylabel('True Positive Rate (Sensitivity)', fontsize=12, fontweight='bold')
    plt.title('H√¨nh 5.2 - ROC Curves So S√°nh 4 M√¥ H√¨nh D·ª± ƒêo√°n Kh√°ch H√†ng Ti·ªÅm NƒÉng', 
              fontsize=14, fontweight='bold', pad=20)
    
    # Thi·∫øt l·∫≠p legend
    plt.legend(loc='lower right', fontsize=11, frameon=True, fancybox=True, shadow=True)
    
    # Thi·∫øt l·∫≠p grid
    plt.grid(True, alpha=0.3, linestyle='--')
    
    # Thi·∫øt l·∫≠p axis
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    
    # Th√™m text box v·ªõi th√¥ng tin
    textstr = f'Dataset: {len(results[list(results.keys())[0]]["y_test"])} samples\nTest Size: 20%'
    props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)
    plt.text(0.02, 0.98, textstr, transform=plt.gca().transAxes, fontsize=10,
             verticalalignment='top', bbox=props)
    
    plt.tight_layout()
    plt.savefig('Hinh_5_2_ROC_Curves.png', dpi=300, bbox_inches='tight', 
                facecolor='white', edgecolor='none')
    print("‚úÖ ƒê√£ l∆∞u H√¨nh 5.2: Hinh_5_2_ROC_Curves.png")
    
    return plt.gcf()

def create_figure_5_3_confusion_matrix(results):
    """T·∫°o H√¨nh 5.3 - Confusion Matrix c·ªßa m√¥ h√¨nh t·ªët nh·∫•t"""
    print("\n=== T·∫†O H√åNH 5.3 - CONFUSION MATRIX ===")
    
    # T√¨m m√¥ h√¨nh t·ªët nh·∫•t d·ª±a tr√™n F1-score
    best_model_name = max(results.keys(), key=lambda x: results[x]['f1_score'])
    best_metrics = results[best_model_name]
    
    print(f"M√¥ h√¨nh t·ªët nh·∫•t: {best_model_name}")
    print(f"F1-score: {best_metrics['f1_score']:.4f}")
    print(f"Accuracy: {best_metrics['accuracy']:.4f}")
    
    # T·∫°o confusion matrix
    cm = confusion_matrix(best_metrics['y_test'], best_metrics['y_pred'])
    
    # T·∫°o bi·ªÉu ƒë·ªì
    plt.figure(figsize=(10, 8))
    
    # T·∫°o heatmap
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['Kh√¥ng ti·ªÅm nƒÉng', 'Ti·ªÅm nƒÉng'],
                yticklabels=['Kh√¥ng ti·ªÅm nƒÉng', 'Ti·ªÅm nƒÉng'],
                cbar_kws={'label': 'S·ªë l∆∞·ª£ng m·∫´u'})
    
    # Thi·∫øt l·∫≠p ti√™u ƒë·ªÅ v√† labels
    plt.title(f'H√¨nh 5.3 - Confusion Matrix c·ªßa M√¥ H√¨nh {best_model_name}\n'
              f'(F1-score: {best_metrics["f1_score"]:.3f}, Accuracy: {best_metrics["accuracy"]:.3f})', 
              fontsize=14, fontweight='bold', pad=20)
    
    plt.xlabel('D·ª± ƒëo√°n', fontsize=12, fontweight='bold')
    plt.ylabel('Th·ª±c t·∫ø', fontsize=12, fontweight='bold')
    
    # Th√™m th√¥ng tin chi ti·∫øt
    tn, fp, fn, tp = cm.ravel()
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    
    # Text box v·ªõi th√¥ng tin chi ti·∫øt
    textstr = f'True Negative: {tn}\nFalse Positive: {fp}\nFalse Negative: {fn}\nTrue Positive: {tp}\n\n' \
              f'Precision: {precision:.3f}\nRecall: {recall:.3f}\nSpecificity: {specificity:.3f}'
    
    props = dict(boxstyle='round', facecolor='lightblue', alpha=0.8)
    plt.text(0.02, 0.98, textstr, transform=plt.gca().transAxes, fontsize=10,
             verticalalignment='top', bbox=props)
    
    plt.tight_layout()
    plt.savefig('Hinh_5_3_Confusion_Matrix.png', dpi=300, bbox_inches='tight',
                facecolor='white', edgecolor='none')
    print("‚úÖ ƒê√£ l∆∞u H√¨nh 5.3: Hinh_5_3_Confusion_Matrix.png")
    
    return plt.gcf(), best_model_name

def create_figure_5_4_feature_importance(results, feature_columns, best_model_name):
    """T·∫°o H√¨nh 5.4 - Feature Importance analysis"""
    print("\n=== T·∫†O H√åNH 5.4 - FEATURE IMPORTANCE ===")
    
    best_model = results[best_model_name]['model']
    
    # L·∫•y feature importance
    if hasattr(best_model, 'feature_importances_'):
        importances = best_model.feature_importances_
    elif hasattr(best_model, 'coef_'):
        # ƒê·ªëi v·ªõi Logistic Regression, s·ª≠ d·ª•ng absolute value c·ªßa coefficients
        importances = np.abs(best_model.coef_[0])
    else:
        print("‚ö†Ô∏è M√¥ h√¨nh kh√¥ng h·ªó tr·ª£ feature importance tr·ª±c ti·∫øp")
        return None
    
    # T·∫°o DataFrame v·ªõi feature names v√† importance
    feature_importance_df = pd.DataFrame({
        'feature': feature_columns,
        'importance': importances
    }).sort_values('importance', ascending=True)
    
    # T·∫°o bi·ªÉu ƒë·ªì
    plt.figure(figsize=(12, 10))
    
    # T·∫°o horizontal bar plot
    bars = plt.barh(range(len(feature_importance_df)), 
                    feature_importance_df['importance'],
                    color=plt.cm.viridis(np.linspace(0, 1, len(feature_importance_df))))
    
    # Thi·∫øt l·∫≠p labels
    plt.yticks(range(len(feature_importance_df)), 
               feature_importance_df['feature'], fontsize=10)
    
    # Thi·∫øt l·∫≠p ti√™u ƒë·ªÅ v√† labels
    plt.title(f'H√¨nh 5.4 - Feature Importance c·ªßa M√¥ H√¨nh {best_model_name}\n'
              f'(T·∫ßm quan tr·ªçng c·ªßa c√°c ƒë·∫∑c tr∆∞ng trong d·ª± ƒëo√°n)', 
              fontsize=14, fontweight='bold', pad=20)
    
    plt.xlabel('T·∫ßm quan tr·ªçng (Importance)', fontsize=12, fontweight='bold')
    plt.ylabel('ƒê·∫∑c tr∆∞ng (Features)', fontsize=12, fontweight='bold')
    
    # Th√™m gi√° tr·ªã importance tr√™n m·ªói bar
    for i, (idx, row) in enumerate(feature_importance_df.iterrows()):
        plt.text(row['importance'] + 0.001, i, f'{row["importance"]:.3f}', 
                va='center', fontsize=9)
    
    # Thi·∫øt l·∫≠p grid
    plt.grid(True, alpha=0.3, axis='x')
    
    # Th√™m th√¥ng tin t·ªïng quan
    total_importance = feature_importance_df['importance'].sum()
    top_5_importance = feature_importance_df.tail(5)['importance'].sum()
    
    textstr = f'T·ªïng importance: {total_importance:.3f}\n' \
              f'Top 5 features: {top_5_importance:.3f} ({top_5_importance/total_importance*100:.1f}%)'
    
    props = dict(boxstyle='round', facecolor='lightgreen', alpha=0.8)
    plt.text(0.02, 0.98, textstr, transform=plt.gca().transAxes, fontsize=10,
             verticalalignment='top', bbox=props)
    
    plt.tight_layout()
    plt.savefig('Hinh_5_4_Feature_Importance.png', dpi=300, bbox_inches='tight',
                facecolor='white', edgecolor='none')
    print("‚úÖ ƒê√£ l∆∞u H√¨nh 5.4: Hinh_5_4_Feature_Importance.png")
    
    # In top 10 features quan tr·ªçng nh·∫•t
    print(f"\nüìä TOP 10 FEATURES QUAN TR·ªåNG NH·∫§T:")
    print("-" * 50)
    top_features = feature_importance_df.tail(10)
    for i, (idx, row) in enumerate(top_features.iterrows(), 1):
        print(f"{i:2d}. {row['feature']:25s} : {row['importance']:.4f}")
    
    return plt.gcf(), feature_importance_df

def create_summary_table(results):
    """T·∫°o b·∫£ng t√≥m t·∫Øt k·∫øt qu·∫£"""
    print("\n=== B·∫¢NG T√ìM T·∫ÆT K·∫æT QU·∫¢ ===")
    
    summary_data = []
    for name, metrics in results.items():
        summary_data.append({
            'M√¥ h√¨nh': name,
            'Accuracy': f"{metrics['accuracy']:.4f}",
            'Precision': f"{metrics['precision']:.4f}",
            'Recall': f"{metrics['recall']:.4f}",
            'F1-score': f"{metrics['f1_score']:.4f}",
            'ROC AUC': f"{metrics['roc_auc']:.4f}"
        })
    
    summary_df = pd.DataFrame(summary_data)
    print(summary_df.to_string(index=False))
    
    # L∆∞u b·∫£ng t√≥m t·∫Øt
    summary_df.to_csv('Chapter5_Model_Summary.csv', index=False, encoding='utf-8')
    print("\n‚úÖ ƒê√£ l∆∞u b·∫£ng t√≥m t·∫Øt: Chapter5_Model_Summary.csv")
    
    return summary_df

def main():
    """H√†m ch√≠nh"""
    print("üéØ T·∫†O C√ÅC BI·ªÇU ƒê·ªí CHO CH∆Ø∆†NG 5 - ƒê√ÅNH GI√Å M√î H√åNH")
    print("=" * 60)
    
    try:
        # 1. T·∫£i v√† chu·∫©n b·ªã d·ªØ li·ªáu
        df = load_and_prepare_data()
        
        # 2. Chu·∫©n b·ªã features
        X, y, feature_columns = prepare_features(df)
        
        # 3. Training c√°c m√¥ h√¨nh
        results = train_models(X, y)
        
        # 4. T·∫°o H√¨nh 5.2 - ROC Curves
        create_figure_5_2_roc_curves(results)
        
        # 5. T·∫°o H√¨nh 5.3 - Confusion Matrix
        fig_cm, best_model_name = create_figure_5_3_confusion_matrix(results)
        
        # 6. T·∫°o H√¨nh 5.4 - Feature Importance
        fig_fi, feature_importance_df = create_figure_5_4_feature_importance(
            results, feature_columns, best_model_name)
        
        # 7. T·∫°o b·∫£ng t√≥m t·∫Øt
        summary_df = create_summary_table(results)
        
        print("\nüéâ HO√ÄN TH√ÄNH T·∫†O C√ÅC BI·ªÇU ƒê·ªí CH∆Ø∆†NG 5!")
        print("=" * 60)
        print("üìÅ C√°c file ƒë√£ ƒë∆∞·ª£c t·∫°o:")
        print("   ‚Ä¢ Hinh_5_2_ROC_Curves.png")
        print("   ‚Ä¢ Hinh_5_3_Confusion_Matrix.png") 
        print("   ‚Ä¢ Hinh_5_4_Feature_Importance.png")
        print("   ‚Ä¢ Chapter5_Model_Summary.csv")
        print("\n‚ú® T·∫•t c·∫£ bi·ªÉu ƒë·ªì ƒë√£ s·∫µn s√†ng ƒë·ªÉ s·ª≠ d·ª•ng trong b√°o c√°o!")
        
    except Exception as e:
        print(f"‚ùå L·ªói: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
