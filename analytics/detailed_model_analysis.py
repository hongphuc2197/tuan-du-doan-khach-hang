#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ph√¢n t√≠ch chi ti·∫øt m√¥ h√¨nh d·ª± ƒëo√°n kh√°ch h√†ng ti·ªÅm nƒÉng
D·ª±a tr√™n dataset th·ª±c t·∫ø t·ª´ website Kyanon Digital
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

def analyze_dataset():
    """Ph√¢n t√≠ch chi ti·∫øt dataset"""
    print("=== PH√ÇN T√çCH CHI TI·∫æT DATASET ===")
    
    # T·∫£i dataset
    df = pd.read_csv('../marketing_campaign_students_627.csv', sep='\t')
    
    print(f"üìä TH√îNG TIN T·ªîNG QUAN:")
    print(f"- T·ªïng s·ªë kh√°ch h√†ng: {len(df)}")
    print(f"- S·ªë features: {len(df.columns)}")
    print(f"- S·ªë kh√°ch h√†ng ti·ªÅm nƒÉng: {(df['Response'] == 1).sum()}")
    print(f"- T·ª∑ l·ªá kh√°ch h√†ng ti·ªÅm nƒÉng: {(df['Response'] == 1).mean()*100:.2f}%")
    
    # Ph√¢n t√≠ch ƒë·ªô tu·ªïi
    df['Age'] = 2024 - df['Year_Birth']
    print(f"\nüë• PH√ÇN T√çCH ƒê·ªò TU·ªîI:")
    print(f"- Tu·ªïi trung b√¨nh: {df['Age'].mean():.1f}")
    print(f"- Tu·ªïi t·ªëi thi·ªÉu: {df['Age'].min()}")
    print(f"- Tu·ªïi t·ªëi ƒëa: {df['Age'].max()}")
    print(f"- ƒê·ªô l·ªách chu·∫©n: {df['Age'].std():.1f}")
    
    # Ph√¢n t√≠ch thu nh·∫≠p
    print(f"\nüí∞ PH√ÇN T√çCH THU NH·∫¨P:")
    print(f"- Thu nh·∫≠p trung b√¨nh: {df['Income'].mean():,.0f} VNƒê")
    print(f"- Thu nh·∫≠p t·ªëi thi·ªÉu: {df['Income'].min():,.0f} VNƒê")
    print(f"- Thu nh·∫≠p t·ªëi ƒëa: {df['Income'].max():,.0f} VNƒê")
    print(f"- ƒê·ªô l·ªách chu·∫©n: {df['Income'].std():,.0f} VNƒê")
    
    # Ph√¢n t√≠ch tr√¨nh ƒë·ªô h·ªçc v·∫•n
    print(f"\nüéì PH√ÇN T√çCH TR√åNH ƒê·ªò H·ªåC V·∫§N:")
    education_counts = df['Education'].value_counts()
    for edu, count in education_counts.items():
        percentage = (count / len(df)) * 100
        print(f"- {edu}: {count} ({percentage:.1f}%)")
    
    # Ph√¢n t√≠ch t√¨nh tr·∫°ng h√¥n nh√¢n
    print(f"\nüíë PH√ÇN T√çCH T√åNH TR·∫†NG H√îN NH√ÇN:")
    marital_counts = df['Marital_Status'].value_counts()
    for status, count in marital_counts.items():
        percentage = (count / len(df)) * 100
        print(f"- {status}: {count} ({percentage:.1f}%)")
    
    # Ph√¢n t√≠ch chi ti√™u
    spending_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']
    print(f"\nüí∏ PH√ÇN T√çCH CHI TI√äU THEO DANH M·ª§C:")
    for col in spending_cols:
        avg_spending = df[col].mean()
        print(f"- {col}: {avg_spending:,.0f} VNƒê")
    
    return df

def create_realistic_evaluation():
    """T·∫°o ƒë√°nh gi√° th·ª±c t·∫ø h∆°n v·ªõi noise v√† validation"""
    print("\n=== ƒê√ÅNH GI√Å M√î H√åNH TH·ª∞C T·∫æ ===")
    
    # T·∫£i dataset
    df = pd.read_csv('../marketing_campaign_students_627.csv', sep='\t')
    
    # Chu·∫©n b·ªã features
    feature_columns = [
        'Year_Birth', 'Income', 'Kidhome', 'Teenhome', 'Recency',
        'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 
        'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases',
        'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 
        'NumWebVisitsMonth'
    ]
    
    # X·ª≠ l√Ω categorical variables
    le_education = LabelEncoder()
    le_marital = LabelEncoder()
    
    df['Education_encoded'] = le_education.fit_transform(df['Education'])
    df['Marital_Status_encoded'] = le_marital.fit_transform(df['Marital_Status'])
    
    feature_columns.extend(['Education_encoded', 'Marital_Status_encoded'])
    
    X = df[feature_columns]
    y = df['Response']
    
    # Th√™m noise ƒë·ªÉ m√¥ ph·ªèng d·ªØ li·ªáu th·ª±c t·∫ø
    np.random.seed(42)
    noise_factor = 0.01  # 1% noise
    X_noisy = X.copy()
    for col in X_noisy.columns:
        if X_noisy[col].dtype in ['int64', 'float64']:
            noise = np.random.normal(0, X_noisy[col].std() * noise_factor, len(X_noisy))
            X_noisy[col] = X_noisy[col] + noise
    
    # Chia d·ªØ li·ªáu v·ªõi stratification
    X_train, X_test, y_train, y_test = train_test_split(
        X_noisy, y, test_size=0.3, random_state=42, stratify=y
    )
    
    # Chu·∫©n h√≥a d·ªØ li·ªáu
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # ƒê·ªãnh nghƒ©a c√°c m√¥ h√¨nh v·ªõi hyperparameters th·ª±c t·∫ø
    models = {
        'Logistic Regression': LogisticRegression(
            random_state=42, 
            max_iter=1000,
            C=1.0,
            penalty='l2'
        ),
        'Random Forest': RandomForestClassifier(
            n_estimators=100, 
            random_state=42,
            max_depth=10,
            min_samples_split=5,
            min_samples_leaf=2
        ),
        'Gradient Boosting': GradientBoostingClassifier(
            n_estimators=100, 
            random_state=42,
            learning_rate=0.1,
            max_depth=6,
            min_samples_split=5
        ),
        'SVM': SVC(
            random_state=42, 
            probability=True,
            C=1.0,
            kernel='rbf',
            gamma='scale'
        )
    }
    
    # Cross-validation setup
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    results = {}
    
    print("üîÑ Training c√°c m√¥ h√¨nh v·ªõi d·ªØ li·ªáu c√≥ noise...")
    
    for name, model in models.items():
        print(f"\nüìà Training {name}...")
        
        # Training
        if name == 'SVM':
            model.fit(X_train_scaled, y_train)
            y_pred = model.predict(X_test_scaled)
            y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
        else:
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            y_pred_proba = model.predict_proba(X_test)[:, 1]
        
        # T√≠nh to√°n metrics
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, zero_division=0)
        recall = recall_score(y_test, y_pred, zero_division=0)
        f1 = f1_score(y_test, y_pred, zero_division=0)
        
        # ROC AUC
        try:
            roc_auc = roc_auc_score(y_test, y_pred_proba)
        except:
            roc_auc = 0.0
        
        # Cross-validation
        if name == 'SVM':
            cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=cv, scoring='accuracy')
        else:
            cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')
        
        cv_mean = cv_scores.mean()
        cv_std = cv_scores.std()
        
        # L∆∞u k·∫øt qu·∫£
        results[name] = {
            'model': model,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'roc_auc': roc_auc,
            'cv_mean': cv_mean,
            'cv_std': cv_std,
            'y_test': y_test,
            'y_pred': y_pred,
            'y_pred_proba': y_pred_proba
        }
        
        print(f"‚úÖ Accuracy: {accuracy:.4f}")
        print(f"‚úÖ Precision: {precision:.4f}")
        print(f"‚úÖ Recall: {recall:.4f}")
        print(f"‚úÖ F1-score: {f1:.4f}")
        print(f"‚úÖ ROC AUC: {roc_auc:.4f}")
        print(f"‚úÖ CV Mean: {cv_mean:.4f} (+/- {cv_std:.4f})")
    
    return results

def create_detailed_comparison_table(results):
    """T·∫°o b·∫£ng so s√°nh chi ti·∫øt"""
    print("\nüìä B·∫¢NG SO S√ÅNH CHI TI·∫æT C√ÅC M√î H√åNH")
    print("=" * 80)
    
    # T·∫°o DataFrame
    comparison_data = []
    for name, metrics in results.items():
        comparison_data.append({
            'M√¥ h√¨nh': name,
            'Accuracy': f"{metrics['accuracy']:.4f}",
            'Precision': f"{metrics['precision']:.4f}",
            'Recall': f"{metrics['recall']:.4f}",
            'F1-score': f"{metrics['f1_score']:.4f}",
            'ROC AUC': f"{metrics['roc_auc']:.4f}",
            'CV Mean': f"{metrics['cv_mean']:.4f}",
            'CV Std': f"{metrics['cv_std']:.4f}"
        })
    
    comparison_df = pd.DataFrame(comparison_data)
    print(comparison_df.to_string(index=False))
    
    return comparison_df

def rank_models(results):
    """X·∫øp h·∫°ng c√°c m√¥ h√¨nh"""
    print("\nüèÜ X·∫æP H·∫†NG C√ÅC M√î H√åNH")
    print("=" * 50)
    
    # T√≠nh ƒëi·ªÉm t·ªïng h·ª£p (weighted score)
    rankings = []
    for name, metrics in results.items():
        # Tr·ªçng s·ªë: F1-score (40%), Accuracy (30%), ROC AUC (20%), CV Mean (10%)
        composite_score = (
            metrics['f1_score'] * 0.4 +
            metrics['accuracy'] * 0.3 +
            metrics['roc_auc'] * 0.2 +
            metrics['cv_mean'] * 0.1
        )
        
        rankings.append({
            'Model': name,
            'Composite Score': composite_score,
            'F1-score': metrics['f1_score'],
            'Accuracy': metrics['accuracy'],
            'ROC AUC': metrics['roc_auc']
        })
    
    # S·∫Øp x·∫øp theo composite score
    rankings.sort(key=lambda x: x['Composite Score'], reverse=True)
    
    for i, rank in enumerate(rankings, 1):
        print(f"{i}. {rank['Model']}")
        print(f"   Composite Score: {rank['Composite Score']:.4f}")
        print(f"   F1-score: {rank['F1-score']:.4f}")
        print(f"   Accuracy: {rank['Accuracy']:.4f}")
        print(f"   ROC AUC: {rank['ROC AUC']:.4f}")
        print()
    
    return rankings

def create_roc_curves(results):
    """T·∫°o ROC curves"""
    print("\nüìà T·∫°o ROC Curves...")
    
    plt.figure(figsize=(10, 8))
    
    for name, metrics in results.items():
        fpr, tpr, _ = roc_curve(metrics['y_test'], metrics['y_pred_proba'])
        plt.plot(fpr, tpr, label=f'{name} (AUC = {metrics["roc_auc"]:.3f})')
    
    plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curves - So s√°nh c√°c m√¥ h√¨nh')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.savefig('roc_curves_comparison.png', dpi=300, bbox_inches='tight')
    print("‚úÖ ƒê√£ l∆∞u ROC curves: roc_curves_comparison.png")

def save_detailed_report(results, comparison_df, rankings):
    """L∆∞u b√°o c√°o chi ti·∫øt"""
    print("\nüíæ L∆∞u b√°o c√°o chi ti·∫øt...")
    
    with open('detailed_model_analysis_report.txt', 'w', encoding='utf-8') as f:
        f.write("B√ÅO C√ÅO PH√ÇN T√çCH CHI TI·∫æT M√î H√åNH D·ª∞ ƒêO√ÅN KH√ÅCH H√ÄNG TI·ªÄM NƒÇNG\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"Ng√†y t·∫°o: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"D·ª±a tr√™n dataset th·ª±c t·∫ø t·ª´ website Kyanon Digital\n\n")
        
        f.write("1. B·∫¢NG SO S√ÅNH CHI TI·∫æT:\n")
        f.write("-" * 50 + "\n")
        f.write(comparison_df.to_string(index=False))
        f.write("\n\n")
        
        f.write("2. X·∫æP H·∫†NG M√î H√åNH:\n")
        f.write("-" * 50 + "\n")
        for i, rank in enumerate(rankings, 1):
            f.write(f"{i}. {rank['Model']}\n")
            f.write(f"   Composite Score: {rank['Composite Score']:.4f}\n")
            f.write(f"   F1-score: {rank['F1-score']:.4f}\n")
            f.write(f"   Accuracy: {rank['Accuracy']:.4f}\n")
            f.write(f"   ROC AUC: {rank['ROC AUC']:.4f}\n\n")
        
        f.write("3. KHUY·∫æN NGH·ªä:\n")
        f.write("-" * 50 + "\n")
        best_model = rankings[0]
        f.write(f"M√¥ h√¨nh t·ªët nh·∫•t: {best_model['Model']}\n")
        f.write(f"L√Ω do: Composite Score cao nh·∫•t ({best_model['Composite Score']:.4f})\n")
        f.write(f"Ph√π h·ª£p cho: D·ª± ƒëo√°n kh√°ch h√†ng ti·ªÅm nƒÉng trong th·ª±c t·∫ø\n")
    
    print("‚úÖ ƒê√£ l∆∞u b√°o c√°o chi ti·∫øt: detailed_model_analysis_report.txt")

def main():
    """H√†m ch√≠nh"""
    print("üîç PH√ÇN T√çCH CHI TI·∫æT M√î H√åNH D·ª∞ ƒêO√ÅN KH√ÅCH H√ÄNG TI·ªÄM NƒÇNG")
    print("D·ª±a tr√™n dataset th·ª±c t·∫ø t·ª´ website Kyanon Digital\n")
    
    try:
        # 1. Ph√¢n t√≠ch dataset
        df = analyze_dataset()
        
        # 2. ƒê√°nh gi√° m√¥ h√¨nh th·ª±c t·∫ø
        results = create_realistic_evaluation()
        
        # 3. T·∫°o b·∫£ng so s√°nh chi ti·∫øt
        comparison_df = create_detailed_comparison_table(results)
        
        # 4. X·∫øp h·∫°ng m√¥ h√¨nh
        rankings = rank_models(results)
        
        # 5. T·∫°o ROC curves
        create_roc_curves(results)
        
        # 6. L∆∞u b√°o c√°o chi ti·∫øt
        save_detailed_report(results, comparison_df, rankings)
        
        print("\nüéâ HO√ÄN TH√ÄNH PH√ÇN T√çCH CHI TI·∫æT!")
        print("T·∫•t c·∫£ k·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u trong th∆∞ m·ª•c analytics/")
        
    except Exception as e:
        print(f"‚ùå L·ªói: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()